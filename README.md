
---

<h1 align="center">
  
  Learn GAN from Scratch

</h1>

----

# ðŸ’¬ How to train:

This is the hard part. Read carefully

1. Generator: given noise generate fake data -> `generated_fake_data`
2. Have some ground truth data i.e True Labels
3. Check what the discriminator says about the fake data coming from step 1: 
    - `Discriminator(generated_fake_data)` -> Real/Fake i.e `Dis_result_for_Gen_output` (probabilistic output)
4. **Train Generator:**  We use the True labels here and don't train the discriminator because we want the generator to make things the discriminator classifies as true. The generator should generate fake data such that they are as close as to real data. So the Discriminator accepts them as Real data. 
    - `Gen_Loss = loss(Dis_result_for_Gen_output, True Label)`
    - `Gen_loss.backward()`
5. **Train Discriminator** 
   1. **On the True data:** The discriminator should be able to identify the real data. Pass in a batch of data from the true data set with a vector of all one labels
    - `Discriminator(True data)` -> i.e `Dis_result_for_True_data`
    - `True_Dis_Loss = loss(Dis_result_for_True_data, True Label)`
   2. **On the Fake Generated data:** Pass our generated data into the discriminator, with `detached weights`, and `zero` labels. Because Discriminator should be able to identify the fake data as zero label. Thus dis_loss is average of `True_Dis_Loss and` and `Fake_Dis_Loss` i.e (`generator_discriminator_loss`).
      ```py
      generator_discriminator_out = discriminator(generated_data.detach())
      generator_discriminator_loss = loss(
          generator_discriminator_out, torch.zeros(batch_size)
      )
      discriminator_loss = (
          true_discriminator_loss + generator_discriminator_loss
      ) / 2
      discriminator_loss.backward()
      ```

> :bulb: The discriminator is trying to learn to distinguish `real` data from `fake` generated data. The labels while training the discriminator need to represent that, i.e. one when our data comes from the real data set and zero when it is generated by our generator.

## :star: Why detach gradients during training Discriminator?

We do this because we are not training the `Generator`. We are just focused on the discriminator. Detaching ensures the Generator gradient will not be updated when  `discriminator_loss.backward()` is executed.

----

## Reference:

- :rocket: [Build a Super Simple GAN in PyTorch](https://towardsdatascience.com/build-a-super-simple-gan-in-pytorch-54ba349920e4)
  - [code repo](https://github.com/nbertagnolli/pytorch-simple-gan)

# ToDO

- [ ] [DCGAN](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
- [ ] [Adversarial Audio Synthesis - WaveGAN](https://paperswithcode.com/paper/adversarial-audio-synthesis) :rocket:

---


